---
title: "오토인코더의 모든 것"
categories: 
  - 단기 강좌
toc: true
---
## 오토인코더의 모든 것 Naver(이활석) 강연자 강의
(https://www.youtube.com/watch?v=o_peo6U7IRM)

### 키워드
- unsupervised learning
- Nonlinear Dimensionality reduction
	- Representation learning
	- Efficient coding learning
	- Feature extraction
	- Manifold learning
  	- 인코더는 차원 축소의 역활
- Generative model learning
	- Variational autoe ncoder 가 나오면서 발전됨
	- 디코더는 생성 모델의 역활
- ML density estimation

## 1. 딥 뉴럴넷을 학습과 관련해서
- **ML  density estimation**을 하는 것
- 기존의 머신러닝과 차이점
  - 기존의 머신러닝 기법은 모델 $f_\theta(x)$ 를 선택함(트리, SVM 등등)
  - 딥 뉴럴넷은 모델 $f_\theta(x)$ 를 딥 뉴럴 네트워크를 사용함
    - 이 때는 Loss function의 제약 조건이 걸리게 됨(MSE, CrossEntropy등밖에 사용 못함)
    - 이는 Backpropagation 알고리즘을 사용하기 때문임
    - 역전파를 하기 위한 두가지 가정
      - 전체에 대한 Loss function은 각 sample별 loss의 **합**과 같다
      - Loss function과 **네트워크의 출력값**과 **정답 값**만 가지고 구성한다
  - Gradient descent를 통해 최적의 파라미터를 구함
    - Iterative method로써 step별로 Loss 값을 줄이는 방향으로 진행
    - 이를 위해서 Tayler expansion을 사용하여 근사하는 방법 사용
      - Sample point에서 약깐만 이동할 때 잘 맞음(Learning rate를 작게 설정하는 이유)
    - 전체 평균을 구하기 어렵기 때문에 일반적으로는 일부(batch)의 gradient를 구해서 진행하는 Stochastic Gradient Descent를 사용함
- Loss function의 해석
  -  Backpropagation관점에서은 cross entropy에서 더 잘 작용함
     - 초기값에 둔감함(Gradient Vanishing Problem이 적음, Activation Function의 미분항이 없기 때문임)
  -  Maximum likelihood관점에는 Discrete는 Cross entropy Continuous는 MSE가 더 잘 맞는다
     -  Cross Entropy는 $p(y|f_\theta (x))$을 Maximize하는 것으로 해석
     - **I.I.D condition 가정**
       - **Independence** : 모든 데이터는 서로서로 독립
  $p(y|f_\theta (x))=\Pi_i p_{D_i}(y|f_\theta (x_i))$
       - Identical Distribution : 우리의 데이터는 동일하게 분포되어 있음
  $\\p(y|f_\theta (x))=\Pi_i p(y|f_\theta (x_i))$
     -  따라서 Density estimation으로 Sampling도 가능함
     - 확률 분포를 Gaussian 분포로 가정 시 MSE값이, Bernoulli 분포로 가정 시 Cross-entropy가 나옴

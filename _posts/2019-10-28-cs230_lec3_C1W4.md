---
title: "cs230_lec3_C1W4"
categories: 
  - cs230 온라인 
toc: true
---
(From : deeplearningai 
https://www.youtube.com/watch?time_continue=386&v=AwQHqWyHRpU)

## Deep L-layer Neural Network
- Shallow model(logistic regression) vs Deep Model 
- Layers number = Hidden layer + Output layer  
- 레이어가 많을수록 더 복잡한 특성을 기억할 수 있음

- **Forward Propagation**
  ![](/assets/img/images/2019-10-31-18-56-25.png)
  - Input $a^{[l-1]}$
  - Output $a^{[l]}, cache (z^{[l]}$
  
- **Backward propagation**
  ![](/assets/img/images/2019-10-31-18-57-03.png)
  - Input $dat^{[l]}$
  - Output $da^{[l-1]}, dW^{[l]}, db{[l]}$
  
- **전체과정(Summary)**
  ![](/assets/img/images/2019-10-31-18-59-21.png)
  - forward propagation는 X로 계속 초기화
  - Backward Propagation은 $da^{[i]} = -\frac{y}{a}+\frac{1-y}{1-a}$

- 각 layer에 대해서는 명시적 반복문을 사용해야 함(Vectorization과 같은 다른 방법이 없으므로)

## 행렬 차원 맞추기
- 코드 디버그할때 행렬의 차원을 적어가면서 확인하면 좋음
- 예제 보면서 연습
  - $z^{[i]}=a^{[i]}=(n^{[i]}, 1)$
  - $w^{[i]}=(n^{[i]},n^{[i-1]})$
  - $b^{[i]}=(n^{[i]},1)$
- 벡터화 했을 때는  
  - $Z^{[i]}=a^{[i]}=(n^{[i]}, m)$
  - $W^{[i]}=(n^{[i]},n^{[i-1]})$
  - $B^{[i]}=(n^{[i]},m))$

## Why Deep representation?
(1) 계층 감지
- 이미지를 예시로 듬
![](/assets/img/images/2019-10-31-19-05-17.png)
  - 첫번째에는 모서리를 감지
    - 모서리 탐지기는 이미지에서 상대적으로 작은 영역을 봄
   - 다음 층에는 감지된 모서리와 그룹화된 모서리를 받아서 부분을 생성할 수 있음
  - 최종적으로 특성을 모아서 다른 종류의 얼굴을 감지할 수 있음
  - 얼굴탐지기는 이미지의 넓은 영역을 볼 수 있음
- 이러한 계층 감지는 이미지뿐만 아니라 음형과 같은 다른 데이터에도 적용됨
  - ex) Audio -> Low load, Ponemes-> Words -> Sentence / Phases
  - 
(2) 회로이론
 -  얇으면 unit이 더 많이 필요
 - Deep 하면 unit이 적게 필요
   - ex) XOR  여러 layer -> O(logn), 하나의 layer -> O($2^{n}$)

## Building block of Deep neural network
 ![](/assets/img/images/2019-10-31-19-06-14.png)
- da^[0]은 지도학습의 가중치를 계산할 때는 필요 없음
- 학습은 forward propagation-> Backward propagation을 거치는 것이 하나의 반복, 이후 파라미터 업데이트
- 정방향 전파, 역방향 전파, 다른 곳으로 정보를 전달하는 캐시로 구성됨

## 파라미터/ 하이퍼 파라미터
 - 파라미터 : W, b
 - 하이퍼 파라미터 : learning rate $\alpha$, # of iteration, # hidden layer L, # hidden units , choice of activation function
    + 모멘텀, 미니배치 크기, 다양한 정규화 매개변수(regularization parameters)
- learning rate 정하기 : cost function이 #에 따라서 얼마나 변화하는지 그래프를 그려보고 alpha 값을 정하기
- Applied Deeplearning은 매우 경험적인 과정이다. 각각 바꿔가면서 사용해봐야함
- **지시사항**
    
    *1*. 새로운 문제를 시작할 때 값의 범위를 시도하고 무엇이 작동하는지 확인해 봐야 함
  
    *2*. 문제에 대해서 진전이 될 대 학습율과 은닉 개수처럼 
  가장 적합한 값들이 바뀔 가능성이 있음( 온라인 데이터 조사).몇 달마다 하이퍼파라미터에 여러 값을 시도하고 더 좋은 값이 있는지 이중으로 확인해 봐라(직관기르는데 도움 줌)
 - 딥러닝이 발전하는 중이기 때문에 이러한 하이퍼파라미터를 설정하는 방법에 대해서 좀더 개발 될 가능성이 있음.

# 딥러닝과 뇌와 관련
- Forward and backward propagation이 뇌의 과정과 비슷하다라는 말
- 다른 뉴런으로부터 신호를 받는 방식이 노드에서 노드로 데이터가 처리되는 과정고 비슷
- 하나의 뉴런은 신경학자들도 잘 모를 정도로 엄청나게 복잡함.
- 앤드류 옹은 딥러닝은 x->y 사이의 매우 복잡한 함수를 찾는 것이라고 생각
- 앞으로 딥러닝을 뇌와 비교하는 경우가 많이 사라져 갈 것
- 컴퓨터 비전 분야에는 인간의 뇌로부터 영감을 많이 받음
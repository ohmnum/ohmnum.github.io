---
title: "cs230_lec6_C5W2"
categories: 
  - cs230 온라인 
toc: true
---
## NLP and Word Embedding

### Word Representation
- 1-hot representation : 한개의 Dictionary 위치값으로 표현하는것
- 다음에 나올 단어들을 서로 연결해보면 다음과 같이 Visualize 가능
![](/assets/img/images/2020-02-11-13-06-47.png)
- T-SNE를 통해 2D로 줄일 수 있음
- 이렇게 관련된 데이터들을 연결하는 것을 **Embedding**이라고 함

## Using word embeddings
- unlabled text를 보면서 embedding을 배울 수 있음
![](/assets/img/images/2020-02-11-13-12-23.png)
  - 두리안을 예로 듬. 두리안은 잘 안쓰이는 단어라 학습이 안되더라도 해당 자리에 들어가므로 과일임을 유추하게 됨
  - 전이 학습을 가능하게 함
- Transfer learning을 하기 위해서는
  - 많은 텍스트 말뭉치에서 단어 embedding을 배우기 or 온라인으로 미리 학습된 embedding을 받기
  - 더 작은 학습 셋을 가지고 있는 새로운 문제에 embedding한 것을 전이해보기
  - optional : 새로운 데이터로 계속 fine tunning 해보기 

## Properties of word embeddings
![](/assets/img/images/2020-02-11-13-30-29.png)
- **Analogy reasoning**을 도와준다
  - 비슷한 분류기준끼리 구분시켜주는 것
![](/assets/img/images/2020-02-11-13-44-52.png)
- Similarity측정

### Cosine Similarity
- Similarity를 측정하는 가장 기본적인 방법
  - Man:Woman as Boy:Girl 같은 문제에서
$$sin(e_w,e_{king}-e_{man}+e_{woman})$$
- 

### Embedding matrix
![](/assets/img/images/2020-02-11-13-58-16.png)
- Embedding matrix E를 아는 것이 목표
- E를 initialize한 뒤에 Gradient descent를 통해서 학습함
- $E /O_j=e_j$를 통해서 word j에 대한 embedding을 얻을 수 있음
- 차원이 크기 때문에 Matrix vector multiplication으로 하기에는 문제가 있음
  - 실제로 적용하기 위해서는 matrix e 의 Column을 보는 특화된 함수를 사용함
  - 수식으로 쓰기에는 위 방식이 편리함

### Learning Word Embedding
- 

### Context/target
- 예측을 하기 전에 있는 단어들인, Context라는 문맥 벡터들이 필요함
- Context의 길이는 자기가 설정하는 것

## Word2vec
- 모두 다 언어 처리 관련 내용이라 생략하겠음
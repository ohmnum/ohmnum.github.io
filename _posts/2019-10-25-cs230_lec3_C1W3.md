---
title: "cs230_lec3_C1W3"
categories: 
  - cs230 온라인 

tags: Deep Learning
use_math: true

toc: true
---
(From : deeplearningai 
https://www.youtube.com/watch?time_continue=386&v=AwQHqWyHRpU)

## Neural Network(신경망)
-  Notaiton $X_1^[1]$
   - Superscript : (1) 번째 데이터를 의미, [1] 번째 층을 의미
   - Subscript : 같은 층의 1번째 node를 의미
-  node들이 여러개 쌓여 있는 것
-  순방향으로 진행하여 Cost function을 구하게 되고 역방향으로 경사값을 구하면서 가충치를 수정하게 된다.
-  구조
   -  Input layer : Input data가 들어가는 부분
   -  Hidden Layer : 내부 입력, 출력이 관찰이 안되는 부분, w b 파라미터들과 관련있음
   -  Output layer : Output이 나오는 node

- 각 node의 연산 구조
  $$z=w^Tx+b$$
  $$a=\sigma(z)$$

  ex) 2 layer 구조에서
  $$z^{[1]}=W^{[1]}x+b^{[1]}$$
  $$a^{[1]}=\sigma(z^{[1]})$$
  $$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$
  $$a^{[2]}=\sigma(z^{[2]})$$

## Vectorization
  ex) 2 layer 구조에서 각 데이터에 대해서 앞에서 배운 과정을 해 주어야 함
  
  **For i to m**
  $$z^{[1](i)}=W^{[1]}x+b^{[1](i)}$$
  $$a^{[1](i)}=\sigma(z^{[1](i)})$$
  $$z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2](i)}$$
  $$a^{[2](i)}=\sigma(z^{[2](i)})$$
![](/assets/img/images/2019-10-28-13-25-49.png)
  **정리하자면 가로로(Horizontally하게)는 데이터 개수, 세로로(Vertically하게)는 데이터의 차원을 넣는다.**
  - 가로축으로는 연산이 병렬적으로 진행되기 때문(다른 가로축이랑 합쳐지지 않음)

## Activation Function (활성화 함수)
 - sigmoid 말고 딴것도 쓸수 있음
   - $a=tanh(z)$ : **Sigmoid**
     - 범위가 0,1 사이라 출력층으로 많이 쓰임
     - z 값이 너무 작거나 너무 크면 함수의 기울기가 0에 가까워져서 경사하강법이 너무 느리게 작동할 수 있다.
   - $a=tanh(z)$ : **Hyperbolic Tangent**
     - 거의 항상 sigmoid보다 좋음
     - 범위가 -1, 1 사이에 있어서 평균값이 0에 더 가까워서 데이터의 중심을 0으로 맞춰주는 효과가 있음
     - z 값이 너무 작거나 너무 크면 함수의 기울기가 0에 가까워져서 경사하강법이 너무 느리게 작동할 수 있다.
   - $a=max(0,z)$ **Relu**
     - 양수일때는 도함수가 1 음수일때는 0이라 경사하강법 계산이 빠름
     - z=0일때 연속이 아니지만 딱 0이 나올 일은 거의 없기 때문에 문제가  발생하지 않음
     - 활성화 함수의 기본값으로 많이 쓰임
     - 단점은 z가 음수일 때 기울기가 0이라는 것
       - 충분한 은닉 유닛의 z은 0보다 크기 때문에 잘 작동함 
       -  이를 보완하는 **leaky relu** 가 있다.   
   - $a=max(0.01z,z)$ **Leaky Relu**
     - Relu의 단점을 보완한 활성화 함수
     - Relu와 큰 차이를 보이진 않지만 성능이 좋다.

- 비선형 함수가 들어가는 이유는 선형 함수를 아무리 결합 해도 선형함수이기 때문이다(node가 하나인 Standard linear regression과 같음)
- 선형 activation 함수는 regression의 output으로만 보통 이용한다.

## Gradient descent (경사하강법)
![](/assets/img/images/2019-10-28-14-22-19.png)
- Forward propagation(정방향 전파)
  - $z^{[1]}=W^{[1]}x+b^{[1]}$
  - $a^{[1]}=g(z^{[1]})$
  - $z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$
  - $a^{[2]}=g(z^{[2]})$
- Back propagaton (역전파)
  - $dz^{[2]}=A^{[2]}-Y$  (Sigmoid 함수일때)
  - $dw^{[2]}=\frac{1}{m}dz^{[2]}A^{[1]T}$
  - $db^{[2]}=\frac{1}{m}np.sum(dz^{[2]},axis=1,keepdims=True)$
    - axis=1은 가로로 더하라는 말
    - keepdims 는 (n,)행렬을 (n,1) 행렬로 출력시키는 명령어
  - $dz^{[1]}=w^{[2]T}dz^{[2]}*g'^{[1]}(z^[1]) \quad by \quad (n^{[1]},m)$
    - *은 성분곱(elementwise product)를 의미
  - $dw^{[1]}=\frac{1}{m}dz^{[1]}X^{T}$
  - $db^{[1]}=\frac{1}{m}np.sum(dz^{[1]},axis=1,keepdims=True)$

- Vectorization 다음과 같이 실시
  ![](/assets/img/images/2019-10-28-14-36-21.png)
  - m으로 나누어주는 이유는 그림과 같이 손실함수를 $\frac{1}{m}$배 하기 때문

## Random Initialization (변수 초기화)
- 변수를 모두 0으로 초기화 하면 경사하강법이 작동 x
  - 그러면 모든 node가 대칭이기 때문, 같은 열마다 같은 값으로 나옴.
- ex)
  - $W^{[1]}=np.random.randn((2,2))*0.01$
    - Q : 0.01과 같이 작은 값을 곱해주는 이유?
    - A : 가중치가 너무 크면 sigmoid, tanh를 할 때 경사하강법이 너무 느리게 작용하게 되기 때문
    - Deep 한 neural network에는 0.01말고 다른 값들도 적용 가능. 다음 시간(**C1W4**)에서 배울 것!
  - $b^{[1]}=np.zero((2,1))$
    - 얘는 대칭성 문제가 없어서 0으로 초기화 해줘도 된다.
---
title: "cs230_lec2_C1W2"
categories: 
  - cs230 온라인
toc: true
---
(From : deeplearningai 
https://www.youtube.com/watch?time_continue=386&v=AwQHqWyHRpU)

## 이중 분류
- X와 Y의 Notation 정의
  - $X.shape=(n_x,m)$ 
  - $Y.shape=(1,m)$
    - $n_x$는 x데이터 총 태그 수
    - m은 해당 데이터 수

## 로지스틱 회귀
- 예측하는 $\hat{y}$는 확률값임
- 시그모이드를 통해 결과를 0~1사이 값으로 출력
![](/assets/img/images/2019-10-25-09-18-23.png)

## 로지스틱 회귀의 Cost Function
- 로지스틱 회귀에서 제곱 오차(MSE)는 Gradient Descent를 못하므로 못씀
- 비슷한 역활을 하면서 Convex인 손실 함수를 정의
$$ L(y,\hat{y})=-(ylog(\hat{y})+(1-y)log(1-\hat{y})) $$
- 이러한 손실 함수를 쓰는 이유
  - y=1일 때 : $L(y,\hat{y})=-log(\hat{y})$ 이므로 $\hat{y}$는 크게 측정됨
  - y=0일 때 : $L(y,\hat{y})=-log(1-\hat{y})$ 이므로 $\hat{y}$는 작게 측정됨
- **Cost function**
  - $J(w,b)=\frac{1}{m}\sum_{i=1}^m L(\hat{y}^{i},y^{i})$

## Gradient descent
- 앞서서 공부한 Cost function을 최소화시키는 w,b 찾기
- step을 반복해서 값이 감소하는 곳으로 이동하는 것
- $\frac{\alpha J(w,b)}{\alpha x}=dw$ 로 정의
- := 는 업데이트 한다는 뜻
$$ w:= w-dw $$

## Derivative
- 함수의 도함수(Derivative)는 함수의 기울기를 의미할 뿐이고, 위치에 따라서 기울기는 달라질 수 있다.

## Logistic Regression Computation Graph
- 신경망 계산은 전방향 전파, 역방향 전파로 나누어짐
  - 전방향 경사를 통해 출력값 결정
  - 역방향 전파를 통해 도함수 값을 구함
- 연쇄 법칙(chain rule) 이용
  ![](/assets/img/images/2019-10-25-11-47-16.png)
  - For 문 쓰는게 데이터 수가 많아지면 문제가 되고 느리기 때문에 Vectorization이 나오게 되었다.

## Vectorization (중요)
- for 문 계산보다 계산이 빠름
- SIMD(Single Instruction Multiple Data)는 CPU,GPU에 있어서 병렬 연산이 가능하게 됨
- GPU는 SIMD 계산을 잘하지만 CPU도 GPU보다는 부족하지만 잘 할 수 있다. 
- 예)
  - np.log(v) : 모두 로그
  - np.abs(v) : 모두 abs
  - np.maximum(v,0)
  - v**2 : 제곱값
  - 1/v : 역
  
- (1) Forward propagation 시 성분 모두 벡터화 해서 계산하기  
  - np.dot(w.T,x) = $w^Tx$
- (2) Back propagation loop에서 for문 없애기
    - $db=\frac{1}{m}\sum_{i=1}^mdz^{(i)}=\frac{1}{m}np.sum(z)$
    - $dw=\frac{1}{m}Xdz^T$ 
  

![](/assets/img/images/2019-10-25-18-24-01.png)

## Broadcasting
- 속도를 빠르게 하는 또다른  Matrix중 하나 
  - A.sum(axis=0) -> 세로축으로 더하란 말
  - A.sum(axis=1) -> 가로축으로 더하란 말
  - ex) percentage = 100*A/cal.reshape(1,A))
    - reshape는 상수 시간이 걸려서 호출이 저렴함
    - (3,4) 벡터를 (1,4) 벡터로 나눔.
  - Broadcasting 다른 예시
    ![](/assets/img/images/2019-10-25-18-41-41.png)

  - **즉 (m,n) 과 (m,1)의 연산[+,-,/,*] 또는 (1,n)와의 연산을 (m,n)으로 바로 하게 해준다**
   
-  Matlab/Octave에서 쓰이는 bsxfun 은 비슷하지만 다른 기능으로 쓰인다.
-  파이썬에 익숙하지 않으면 찾기 힘든 오류가 잘 발생
  - !실수하기 쉬운 것부분 1! a=np.random.randn(5) 라고 하면
    - a.shape=(5,)로 나옴.
      - A^T는 A와 같게 나옴 조심
      - a.T는 (1,5) 벡터로 출력됨
      - np.dot(A,A^T)는 자료의 외적이 나오게 됨
      - a= np.random.randn(5,1) 처럼 랭크 1 배열 뽑기 말기뽑기 
    - **(5,) 와 같은 rank 1 array 쓰지 말자**
    
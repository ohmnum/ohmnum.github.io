---
title: "기계학습_lec2_AAILab"
categories: 
  - 기계학습_AAILab 
toc: true
---
참고- 문일철 교수님 유튜브-AAiKaist \
(https://www.youtube.com/watch?v=sDG1Y1vxOjs&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=2)

(새롭게 알게된 정보들만 자세히 기술, 나머지 내용들은 간략하게 정리함)


# Fundamentals of Machine Learning
- Learning From Experience (E)
- With respect to some class of tasks (T)
- And performance Measure Improves(P)
# Lecture Flow
- 몇 강의간 해당 방식의 공부로 진행될 예정
  - **Rule Based Machine Learning** Technique
  - **Decision Tree** 
  - **Statistic based Machine Learning** Technique
    -  Naive Bayes Filter


## Rule Based Machine Learning
- Perfect world 가정
  - 관측 에러가 없음, 다 일관적임. 변칙이 없음
  - 변수들이 통계적인 변수들이 아님
  - 시스템을 재구성할 수 있는 모드 정보가 관측값에 들어 있음
### Function Approximation
 - c(x)=H(x)를 만드는 것  
  - Instance : X
  - Training Dataset : D
  - Hypothesis : H
  - Target Function : c
- 다시 말하면 hypothesis space 와 Input space를 mapping하는 것
  - Hypothesis에 따라서 **general** 한것인지 **specific** 한것인지 정해짐

### Find-S Algorithm
- 가장 specific hypothesis를 잡음(ex. Null hypothesis, 모두 다 부정하는 가설)
- D에 속하는 모든 x에 대해서 검정해보면서 모든 피쳐가 H 가설에 속하도록 업데이트 하는 방식

### Version Space
- 모든 hypothesis는 가능하지만 convergence를 보이는 것을 찾기는 어려움
- 특정 가설을 찾는 것보다 가설의 범위를 찾는 것으로 정해주는 것(Version Space, VS)
  - General Boundary (G)
  - Specific Boudnary (S)
  - Every hypothesis **h** satisfies
    $$VS_{H.D}=\{ h \in H | \exists s \in S, \exists g \in G, g \geq h \geq s \}$$

### Candidate Elimination Algorithm
- Version space를 만들기 위해서 G와 S에 대해서 배우는 것
- Null space : 인스턴스에 있는 것만 커버할 수 있도록 generalize
  - 상충되는 조건이 있다면 없애줌
- General Space : 인스턴스에 있는 것들을 커버할 수 있도록 specific하게 해주는 것
  - 상충되는 조건이 있다면 없애줌
- 교수님이 들어주시는 예시를 보면서 확인하면 이해 잘 할 수 잇음
- 문제점
  - 새로운 예시들에 대해 분류가 가능하지만 분류가 가능하지 않은 예시도 생김( G : O, S : X )
  - 따라서 실제 상황에 적용하기에는 문제가 있을 수 있음
- 완벽한 World 에서는 **Converge**하고 **Correct**한 알고리즘임
- 그러나 우리가 사는 세상에서는
  - 데이터의 노이즈 존재
  - 우리가 든 Decision Factor말고 다른 것도 존재할 수 있음

## Decision Tree
- 노이즈에 robust한 알고리즘 개발이 요구됨
- Hypothesis를 간결히 표현하는게 요구됨
- Rule 기반 알고리즘에 가까움

### Entropy
- 여러 특성 중에서 어떤 특성을 기준으로 확인할 것인지 선별해줌
- 문제 상황에서 불확실성을 줄이기 위해서 도입 
- 높은 엔트로피는 더 많은 불확실성을 의미
$$H(X) = -\sum_X P(X=x)log_b P(X=x)$$
  - X는 Discrete variable임, continuous variable일 때는 Integral로 바꾸면 됨
- **Conditional Entropy**
  - 어떠한 조건이 있을 때 엔트로피
$$H(Y|X)=\sum_X P(X=x)H(Y|X=x)$$
$$=\sum_x P(X=x)\{-\sum_Y P(Y=y|X=x)log_b P(Y=y|X=x)\}$$ 
  - 어떠한 조건 : X의 조건을 의미함
- **Information Gain**
  - 조건을 줬을 때의 정보량 차이
$$IG(Y,A_i)= H(Y)-H(Y|A_i)$$

### Top-Down Induction Algorithm
- Information gain이 높은 변수를 선택해서 root를 만듬
- Information gain을 통해서 Best variable를 찾기
- 선택된 Variable에 대해서 
  - 예시들을 Sorting하기
  - Sorting 한 아이템들을 변수의 값에 적용
- 더욱 더 정확하게 하려면 Decision tree를 계속해서 더 만들면 됨

### Problem of Decision Tree
- Decision Tree가 점점 커질수록 새로운 데이터에 대해서는 잘 맞지 않음
- 실제 상황에서는 노이즈도 많고 비일관성이 높아서 잘 맞지 않음 

## Linear Regression
- Hypothesis(가설)을 Function의 형태로 만드는 것
- 변수에 가중치를 주고 이를 Linear Sum하는 것
  $$\hat{f}(x;\theta)=\theta_0+\Sigma_{i-1}^{n}\theta_ix_i=\Sigma_{i=0}^{n}\theta_ix_i$$
- 실제 데이터는 노이즈가 있기 때문에
$$ f(x;\theta)=\Sigma_{i=0}^{n}\theta_ix_i+e$$
- $\theta$값을 추정해야 함
 $$\hat{\theta}=argmin_\theta (f-\hat{f})^2=argmin_\theta (Y-X\theta)^2$$
 $$\hat{\theta}=(X^TX)^-1X^TY$$
- 실제 상황에서는 선형회귀할 때 잘 안맞을 수 있음
- 이때 $x_i$를 변환하는 $\phi$함수를 정의 
$$ h : \hat{f}(x;\theta)=\Sigma_{i=0}^{n}\Sigma_j=1^m \theta_{i,j}\phi_j(x_i)$$
- 심플한 방식이며 기본적인 모델, 실제 상황에도 얼추 잘 적용
- 많은 데이터가 들어올수록 잘 맞지 않는 한계점이 드러남
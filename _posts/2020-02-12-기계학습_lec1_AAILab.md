---
title: "기계학습_lec1_AAILab"
categories: 
  - 기계학습_AAILab 
toc: true
---
참고- 문일철 교수님 유튜브 (https://www.youtube.com/watch?v=sDG1Y1vxOjs&list=PLbhbGI_ppZISMV4tAWHlytBqNq1-lb8bz&index=2)

(새롭게 알게된 정보들만 자세히 기술, 나머지 내용들은 간략하게 정리함)


## 머신 러닝의 적용분야
- Document classification
  - spam filter같은 것
  - 요즘은 중요도 관련해서 태그를 달기도 함
- Sentiment 분석
  - Social site data를 통해서 사람들의 경향을 summarize할 수 있음
- Stock market Prediction and more
- etc

### 학습의 종류
- 지도학습
- 비지도학습
  - 군집화
  - Latent factor
  - 노이즈 필터링
- 강화학습
  - 해당 강좌에는 이 부분은 다루지 않을 예정

## 확률의 이해
- 회장님 압정던지기 썰로 설명해주심(재밌음)
- 단순히 5번던져서 3번 Tail이 나오면 앞면이 나올 확률을 3/5로 말해도 되는가?
- 명쾌한 설명은 다음 분포를 알아야 됨
### Binominal Distribution
- Discrete probability distribution
- IID 조건에서 진행
  - 독립된 시행을 계속해서 실행할 때의 확률
$$P(D|\theta)=\theta^{a_H}(1-\theta)^{a_T}$$
- 가장 적절한 Theta를 찾는법 
  - **Maximum Likelihood Estimation(MLE)**로 파라미터 추정 가능
    $$\hat{\theta}=argmax_{\theta} \ P(D|\theta)$$
    - 로그 이용해서 쉽게 구할 수 있음
  $$\hat{\theta}=\frac{a_H}{a_H+a_T}$$

- 계속해서 50번 던졌을때도 Tail 30번 나옴. 위와 같은 추가적인 시행으로 달라지는 것은?

### Simple Error Bound
- 우리가 구한 것은 추정치
- 즉 시행을 계속하면 Error을 더 줄여줌
- **Hoeffding's inequality**에 따르면
  $$P(|\hat{\theta}|\geq \epsilon) \leq 2e^{-2N\epsilon^2}$$
- 역으로 오차를 특정 값으로 줄이는데 필요한 N을 구할 수도 있음

### Probability Approximate Correct(PAC) Learning
- 지금까지 진행한 과정들을 의미
- "아마도 이 오차 범위에 추정치는 이런 확률로 맞을꺼야" 추정하는 것

## MAP!
- 앞의 압정 이야기에서 이어짐
- Bayes는 사전정보가 존재하면 해당 정보또한 포함해서 확률을 알아보자라고 함
 $$P(\theta|D)= \frac{P(D|\theta)P(\theta)}{P(D)}$$
 $$Posterior = \frac{Likelihood \times Prior\ knowledge}{Normalizing\ Constant}$$

- 즉 문제를 풀기 위해 다시 정리하면
 $$P(\theta|D) \propto P(D|\theta)P(\theta)$$
- beta distribution을 사용하는건 어떨까요?
  - 0~1사이로 잘 confined 된 CDF
  $$P(\theta)=\frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\Beta(\alpha,\beta)}, \Beta(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$$
  $$\Gamma(\beta)=(\beta-1)!

- 즉 위의 분포로 구해보면
$$P(\theta|D) \propto \theta^{a_H+\alpha-1}(1-\theta)^{a_T+\beta-1}